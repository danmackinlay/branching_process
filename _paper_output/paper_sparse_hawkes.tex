
% Default to the notebook output style

    


% Inherit from the specified cell style.




    

\documentclass[11pt]{article}


    
    

    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    
\usepackage{apacite}


    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    
\title{Regularised parameter selection in the non-stationary Hawkes process}

    



    

\author{Daniel MacKinlay}

    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    

    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    


    \begin{document}
    
    
    \maketitle
    
    

    
    \section{\texorpdfstring{\(\ell_1\)-penalised non-stationary Hawkes
branching
process}{\textbackslash{}ell\_1-penalised non-stationary Hawkes branching process}}\label{ell_1-penalised-non-stationary-hawkes-branching-process}





    \subsection{Abstract}\label{abstract}

We are interested in the problem of simultaneous identification of the
parameters of a Hawkes point process and identifying a sparse driving
signal by means of \(\ell_1\) penalised regression where, by assumption,
the driving noise has a different form. This is analaogous to a blind
deconvolution problem, robust regression, or a compressive sensing-style
inverse problem, but considerably more challenging; While the process
model superfcially resembles a linear system, there are few of the
convenient results that would apply in a linear regression problem; and
as a time series problem it is difficult to choose our penalty
parameters by standard non-parametric means such as cross validation.
The proposed method uses a heuristically-motivated degrees-of-freedom
regularisation parameter selection, which we test through simulation.
The proposed method works well in practice.

    \subsection{Introduction}\label{introduction}

We are given a counting process \(N\) on \(\mathbb R\), and an interval
\([0,T]\). (TODO clarify fit interval versus data interval)

We write the time series either by its values
\(\mathbf{t}=\{t_1, t_2,\dots,t_N(T)\}\). We represent the time series
either by its values or its arrival times as notationally conveneient.
We call it a (TODO) predictable Poisson point process if it has an
associated \emph{intensity} process \(\lambda\), such that \(\lambda\)
is predictable with respect to the filtration generated by \(N\), i.e.
\cite{daley_introduction_2003}

\[\lambda(\mathcal{F(t^-)}):=\lim_{h\to 0} \frac{\mathrm E\left(N(t,t+h)\right)}{h}.\]

    The Hawkes process \cite{hawkes_point_1971} in particular, is the
linear, self-exciting point process; Concretely, given a "background"
rate \(\mu:\mathbb{R}^+\rightarrow\mathbb{R}^+,\) and a non-negative
influence kernel with positive support
\(\phi:\mathbb{R}\rightarrow\mathbb{R}^+\cap\{0\}\), the Hawkes process
has the following expression for its intensity:

\[
\begin{aligned}
\lambda(\mathcal{F(N(t))}^-_{(-\infty,t)}) &= \mu(t) + \phi * N\\
&= \mu(t)  + \int_{-\infty}^{\infty}\phi(t-s)dN(s)\\
&= \mu(t) + \int_{-\infty}^{t}\phi(t-s)dN(s)\\
&= \mu(t) + \sum_{t_i\lt t}\phi(t-t_i)
\end{aligned}
\]

For brevity, we write
\(\lambda(\mathcal{F(t)}^-_{(-\infty,N(t))})=\lambda(t)\).

    The log-likelihood function for a given realisation of the data is given
\cite{ozaki_etimating_1979}

\[L_\pi(N;\mu,\phi):=-\int_0^T\lambda(t;\mu,\phi)dt + \int_0^T\log \lambda(t;\mu,\phi) dN_t \]

Using the method of maximum likelihood we estimate our parameter vector
by numerical optimisation of
\[\hat{\theta}_\pi(N) = \operatorname{argmax}_{\mu,\phi} L_\pi(N;\theta).\]

    We take \(\mu\) to be an unknown sparse, non-random signal in a
particular sense. We will assume that, given a basis
\(\{\psi_j \}_{1\leq j\leq M}\), that we may write the background rate
as \[\mu(t) = \mu_0 + \sum_{j=1}^M  \omega_j \psi_j(t).\]
\emph{Sparsity} here will imply that the coefficients \(\omega_j\) will
be mostly 0.

In practice, we choose
\[\psi_j(t) := \mathbb{I}_{[\tau_{j},\tau_{j+1})}(t)\] for some lattice
\(\{\tau_j\},\) which will correspond to the assumption that "sparse"
triggers will have localised support in time; we are looking for short
"spikes".

Similarly, we will approximate \(\phi\) in a different functional basis
\(\{\bar{\psi}\}_{1\leq k \leq K}\), such that
\(\phi(t):= \sum_{k=1}^K \bar{\psi}(t;k)\). This basis choice is likely
arbitrary, but many exist in the literature trading off computational
and statistical convenience, including Laguerre polynomials
\cite{ogata_linear_1982}, Exponential functions
\cite{schoenberg_consistent_2005,rambaldi_modeling_2015}, and step
functions \cite{eichler_graphical_2016} As a convenient choice, we
propose a basis of smooth unimodal non-negative functions with a
computationally convenient form, the Maxwell density,
\[\bar{\psi}_j(t;\sigma):=\sqrt{\frac{2}{\pi}} \frac{t^2\exp\left(-\frac{t^2}{2\sigma^2}\right)}{\sigma^3}.\]
so that

\[\phi(t):= \sum_{j=1}^K \kappa_j\bar{\psi}(t;\sigma_j).\] Once again,
some vector \(\{\sigma_j\}_{j\leq K}\) of time-scale parameters are
chosen \emph{a priori}.



    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{paper_sparse_hawkes_files/paper_sparse_hawkes_10_0.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    Since this is a general nonparametric problem we will use traditional
regularisation techniques to control estimates. We hope to recover the
unknown excitation kernel and a background intensity by appropriate
penalisation of the terms.

For the penalized log likelihood for penalty functional \(\pi\), we
write

\[L_\pi(N, \mu, \phi):= L(N; \mu, \phi)- \pi(\mu, \phi)\]

As in plain log-likelihood optimization, the estimate is defined as the
maximizer:

\[\hat{\theta}_\pi(N) = \operatorname{argmax}_\theta L_\pi(N;\theta)\]

    A motivation for these specific choices is a particular real-world
problem: modelling viral sharing and spreading in social media. We
suspect that online popularity is governed by some mixture of two
componenets:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  organic social sharing, modeled by the self-exciting \(\phi\)
  component, and
\item
  intermittent external boosts represented as variations in the the
  \(\mu\) component, such as when an internet meme is featured on an
  external medium such as television.
\end{enumerate}

Models such as the Hawkes process have frequently been applied in an
attempt to disambiguate these two
\cite{crane_robust_2008,mitchell_hawkes_2010,rizoiu_expecting_2017}.

    \(\ell_1\) type penalties are known for their ability to produce sparse
solutions, so we choose penalties of the form
\[\pi(\theta)=p\left\|\{\sigma_j\}\right\|_1+q\left\|\{\omega_j\}\right\|_1.\]
With the right choice of the parameters \(p\) and \(q\) we hope to
shrink unnecessary parameters to zero, identifying predictively
necessary ones, as in Lasso selection.

    There is no closed-form solution for the optimum of the Hawkes model
likelihood, penalised or not. The derivatives of the unpenalised
likelihood are easy to calculate manually -\/-\/-
\cite{ozaki_maximum_1979}. Here, however, we use standard reverse-mode
automatic differentiation \cite{baydin_automatic_2014}. The optimisation
proceeds using a standard gradient-based black-box based Truncated
Newton algorithm; as more specialised sparse regression algorithms such
as Least-Angle Regression, do not directly apply here.

    \section{Hyperparameter selection}\label{hyperparameter-selection}

    \subsubsection{Degrees of freedom}\label{degrees-of-freedom}

Whilst we might wish to select the regularisation penalties by cross
validation, cross validation is not straightforward for time series.

The traditional means of performing model selection in Hawkes-type
models e.g. \cite{ogata_estimation_1983}, uses \emph{degrees-of-freedom}
penalties \cite{efron_how_1886}. Specifically, Akaike's
\cite{akaike_likelihood_1981} Information Criterion, or \emph{AIC}, is
widely popular.

The AIC formula, for a given log-likelihood parameter estimate
\(\hat{\theta}(N)\), with 'degrees of freedom', \(d\) is given

\[\mathrm{AIC}(N,\hat{\theta}(N))=2d-2L(N,\hat{\theta}(N))\]

The degrees of freedom for a parametric model is taken to be the number
of free parameters, equal to the cardinality of the parameter vector
\(\theta\).

    The AIC does not immediately solve the problem of regularized parameter
selection where the regularisation parameter may be a real value
smoothly interpolating many possible models. One can calculate an AIC
for penalised models, but this require onerous Hessian matrix inversions
and becomes costly in high dimensions, and in any case does not
generalise readily to non-smooth penalties.
\cite{konishi_information_2008}

We proceed heuristically, based on the degrees-of-freedom arguments from
Lasso-type linear-model regularisation. There, the number of non-zero
coefficients fit under a particular value of the regularization penalty
\(\pi\) in an \(\ell_1\) regression model provides an unbiased estimate
of the effective degrees of freedom of the model
\cite{zou_degrees_2007}.

    This suggests, by analogy, an AIC estimate based on the number of
non-zero parameters for given regularization coefficients:

\[
\hat{d}\left(N,\hat{\theta},p,q\right)=\sum_j\mathbb 1\left\{\hat{\theta_j}(N,p,q)\neq 0\right\}
\]

    The suggested combined procedure is to combine, in an inner loop, the
fitting of model parameters for given values of \(p\) and \(q\), and
selecting the hyperparameters with the best overall AIC value.

    \subsection{Simulations}\label{simulations}

    TODO: switch to consistent step-function display of instantaneous rate.

    We simulate data with parameters to match the stylized behaviour of the
social media time series of previous work
\cite{crane_robust_2008,mitchell_hawkes_2010,rizoiu_expecting_2017}, to
test the efficacy of the proposed method.



    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{paper_sparse_hawkes_files/paper_sparse_hawkes_23_0.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{paper_sparse_hawkes_files/paper_sparse_hawkes_23_1.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{paper_sparse_hawkes_files/paper_sparse_hawkes_23_2.pdf}
    \end{center}
    { \hspace*{\fill} \\}
    
    Extensive grid search for optimsal parameters is expensive, so we select
both regularisation parameters using optimal sequential sampling by
Gaussian process regression of the combined objective function
\cite{snoek_practical_2012} using the software package
\href{https://github.com/scikit-optimize/scikit-optimize}{scikit-optimize}.


\texttt{\color{outcolor}Out[{\color{outcolor}27}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{paper_sparse_hawkes_files/paper_sparse_hawkes_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    With a single regularisation parameter, only for the background rate
coefficient \(\omega_j\), we recover the background rate efficiently.


\texttt{\color{outcolor}Out[{\color{outcolor}28}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{paper_sparse_hawkes_files/paper_sparse_hawkes_27_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \subsection{How to simulate a Hawkes
process}\label{how-to-simulate-a-hawkes-process}

Here we follow a standard "thinning" method of simulating Hawkes
processes, given as Algorithm 7.5.IV of Daley and Vere-Jones'
\emph{Introduction to the Theory of Point Processes Volume 1}
\cite{daley_introduction_2003}, restating Ogata's original Algorithm 2
(\cite{ogata_lewis_1981}.

Since that is a general case, here we present a specific version for
linear Hawkes processes which is considerably simpler.

For clarity, we will use the representation of the Hawkes process as an
immigrant-birth process, where the recursive "self-excited" component
corresponds to the "births". We will assume that we are given a list of
arrival times of immigrants, \(\{a_1,a_2,\dots,a_N\}\), and wish to
simulate the births arising from these immigrants. We will associate a
point and its arrival time and discuss them interchangeably where
ambiguity does not arise.

Each of these \(a_i\) gives rise to a family of descendants
\(\mathrm{descendants}(a_i)\), distributed independently of all other
families, whose combined superposition is the final Hawkes process
(\cite{hawkes_cluster_1974}. It will suffice to simulate the arrival
times of each of these families. Without loss of generality we may
consider the problem of simulating the births in a single family started
at time \(a_1=0\) and translate it.

Now the families
\(\mathrm{descendants}(t_j)\forall t_j\in \mathrm{children}(a_i)\) are
all once again identically and mutually independently distributed.
Proceeding by induction, it suffices to simulate the children of a
single point at time \(0\). The same method will suffice for its
children, children of children and so on. The final realisation can be
constructed from these simualtions by appropriate concatenation and
translation of the generations of each family.

The single kernel simulation is simple, since the Poisson intensity
process is no longer conditional on the history of the sequence. As
before, the influence kernel is \(\phi\). We will assume that \(\phi\)
is a.e. continuous \(L_1\) integrable and eventually decreasing in
\(t\). We associate with \(\phi\) a "majorant", \(\phi^{**}\) such that
\[\phi^{**}(t)\geq\phi(t), \forall t>0\] and \(\phi^{**}\) is
non-increasing and integrable. With the unimodal influence kernels used
in the current paper, such majorants are trivial to construct.

We will use the fact that there is some time \(T>0\) such that we do not
consider events after it, i.e. that we only care about arrival times on
\([0,T]\), and so only children that occur before time \(T\) can be
relevent.

\subsubsection{Algorithm: Ogata's thinning
algorithm}\label{algorithm-ogatas-thinning-algorithm}

Our goal is to simulate the set \(S=\mathrm{children}(a_0)\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set \(i=1, t=0\), \(S=\varnothing\)
\item
  Generate \(\Delta\sim \operatorname{Exp}(1/\phi^{**}(t))\)
\item
  Set \(t\leftarrow t+\Delta\)
\item
  If \(t\geq T\), halt.
\item
  With probability \(P=\frac{\phi(t)}{\phi^{**}(t)}\), set
  \(S\leftarrow S\cup \{t\}\).
\item
  Continue from step 2.
\end{enumerate}

    \subsection{Conclusions}\label{conclusions}

    

    \subsection{Appendix and nonsense}\label{appendix-and-nonsense}


    Given two ML fitted models, \((M_1,\hat{\theta}^M_1)\) and
\((M_2\,\hat{\theta}^M_2)\), the difference in their AIC values is an
estimate of the relative Kullback-Leibler divergence of the inferred
measures \(\hat{\mu_1},\hat{\mu_2}\) from the unknown true distribution,
\(\mu\) i.e.

\[\mathrm{AIC}(X,M_1)-\mathrm{AIC}(X,M_2) \simeq D_\mathrm{KL}(\mu\|\hat{\mu_1}) - D_\mathrm{KL}(\mu\|\hat{\mu_2})\]

    mean delay \(\mu=2\tau \sqrt{\frac{2}{\pi}}\)

mode \(\sqrt{2}\tau\)

scale \(\sigma=\sqrt{\frac{\tau^2(3 \pi - 8)}{\pi}}\)

We take fixed \(M\)-point lattice \(\tilde{\tau}\) and \(K\)-fixed
scales \(\tilde{\sigma}.\)

The parameter vector to be estimated then is
\(\theta:=(\mu, \tilde{\omega}, \tilde{\kappa}).\)

    The AIC is based on asymptotic arguments, and Sugiura's
finite-sample-corrected version \emph{AICc}, has lower bias for most
practical applications \cite{sugiura_further_1978}.

\[{\mathrm {AICc}}:={\mathrm {AIC}}+{\frac {2d(d+1)}{N_T-d-1}}\]

    For non-parametric extension to a parametric model, one usually
penalizes only the non-parametric extensions to the model, such that
they vanish as the penalty term increases. \cite{green_penalized_1987}

\[\lim_{\pi\to\infty}\hat{\theta}_\pi'(\mathbf t) =\hat{\theta}(\mathbf t)\]

I follow this practice here, penalizing only the deviations of
\(\omega(t)\) from the parametric estimate.

\[L_\pi(\mathbf t;\mu,\phi):=-\int_0^T\lambda(t;\mu,\phi)dt + \int_0^T\log \lambda(t;\mu,\phi) dN_t - \pi\|\boldsymbol \omega_\theta(t)\|_1\]


    % Add a bibliography block to the postdoc
    
    

\bibliographystyle{apacite}
\bibliography{refs}

    
    \end{document}
